{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMSoliQEPuh9a0ApDwk5oeI"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"error","timestamp":1733087037942,"user_tz":-330,"elapsed":45744,"user":{"displayName":"Praneet Avhad","userId":"11641365352815720975"}},"outputId":"57299258-748c-4db3-8d1f-30da8251e25a","id":"mHHdNJEdCn0l"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting python-dotenv\n","  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n","Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n","Installing collected packages: python-dotenv\n","Successfully installed python-dotenv-1.0.1\n","Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.5.3)\n","Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n","Requirement already satisfied: gdown in /usr/local/lib/python3.10/dist-packages (5.2.0)\n","Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown) (4.12.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown) (3.16.1)\n","Requirement already satisfied: requests[socks] in /usr/local/lib/python3.10/dist-packages (from gdown) (2.32.3)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gdown) (4.66.6)\n","Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown) (2.6)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.4.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2024.8.30)\n","Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (1.7.1)\n","/usr/local/lib/python3.10/dist-packages/gdown/__main__.py:140: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n","  warnings.warn(\n","Downloading...\n","From (original): https://drive.google.com/uc?id=1JYOFdBSnKQcB24tnQY0HzwsJB4nf2Eek\n","From (redirected): https://drive.google.com/uc?id=1JYOFdBSnKQcB24tnQY0HzwsJB4nf2Eek&confirm=t&uuid=5c14a589-77c8-440f-af40-df480747f796\n","To: /content/Combined_Flights_2021.csv\n","100% 2.21G/2.21G [00:14<00:00, 152MB/s]\n","########################## Problem 1 ########################\n"]},{"output_type":"error","ename":"Py4JJavaError","evalue":"An error occurred while calling o27.partitions.\n: org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: file:/content/flights_data.txt\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:304)\n\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:244)\n\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:332)\n\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:208)\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:294)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:290)\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:294)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:290)\n\tat org.apache.spark.api.java.JavaRDDLike.partitions(JavaRDDLike.scala:61)\n\tat org.apache.spark.api.java.JavaRDDLike.partitions$(JavaRDDLike.scala:61)\n\tat org.apache.spark.api.java.AbstractJavaRDDLike.partitions(JavaRDDLike.scala:45)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: java.io.IOException: Input path does not exist: file:/content/flights_data.txt\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:278)\n\t... 25 more\n","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-b6e7a205691b>\u001b[0m in \u001b[0;36m<cell line: 220>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 221\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-2-b6e7a205691b>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    183\u001b[0m     \u001b[0;31m# read the file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m     \u001b[0mflights_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtextFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mairline_textfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m     \u001b[0msorted_airline_pairs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mco_occurring_airline_pairs_by_origin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflights_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m     \u001b[0msorted_airline_pairs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpersist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mpair\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted_airline_pairs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-2-b6e7a205691b>\u001b[0m in \u001b[0;36mco_occurring_airline_pairs_by_origin\u001b[0;34m(flights_data)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;31m# Step 2: Group airlines by (date, origin)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;31m# Output: RDD[((date, origin), [airline1, airline2, ...])]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m     \u001b[0mgrouped_airlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsed_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupByKey\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapValues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;31m# Step 3: Generate airline pairs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/rdd.py\u001b[0m in \u001b[0;36mgroupByKey\u001b[0;34m(self, numPartitions, partitionFunc)\u001b[0m\n\u001b[1;32m   4177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4178\u001b[0m         \u001b[0mlocally_combined\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcombine\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreservesPartitioning\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4179\u001b[0;31m         \u001b[0mshuffled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlocally_combined\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartitionBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumPartitions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartitionFunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4181\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mgroupByKey\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIterable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mV\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mIterable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mV\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/rdd.py\u001b[0m in \u001b[0;36mpartitionBy\u001b[0;34m(self, numPartitions, partitionFunc)\u001b[0m\n\u001b[1;32m   3844\u001b[0m         \"\"\"\n\u001b[1;32m   3845\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnumPartitions\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3846\u001b[0;31m             \u001b[0mnumPartitions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_defaultReducePartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3847\u001b[0m         \u001b[0mpartitioner\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPartitioner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumPartitions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartitionFunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3848\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartitioner\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mpartitioner\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/rdd.py\u001b[0m in \u001b[0;36m_defaultReducePartitions\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   4865\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaultParallelism\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4866\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4867\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetNumPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4868\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4869\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mlookup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"RDD[Tuple[K, V]]\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mV\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/rdd.py\u001b[0m in \u001b[0;36mgetNumPartitions\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   5451\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5452\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgetNumPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5453\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prev_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5454\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5455\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n","\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o27.partitions.\n: org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: file:/content/flights_data.txt\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:304)\n\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:244)\n\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:332)\n\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:208)\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:294)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:290)\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:294)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:290)\n\tat org.apache.spark.api.java.JavaRDDLike.partitions(JavaRDDLike.scala:61)\n\tat org.apache.spark.api.java.JavaRDDLike.partitions$(JavaRDDLike.scala:61)\n\tat org.apache.spark.api.java.AbstractJavaRDDLike.partitions(JavaRDDLike.scala:45)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: java.io.IOException: Input path does not exist: file:/content/flights_data.txt\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:278)\n\t... 25 more\n"]}],"source":["!pip install python-dotenv\n","!pip install pyspark\n","!pip install gdown\n","\n","import os\n","from itertools import permutations\n","from dotenv import load_dotenv\n","from pyspark import RDD, SparkContext\n","from pyspark.sql import DataFrame, SparkSession\n","\n","### install dependency ###\n","# pip install python-dotenv\n","# pip install pyspark # make sure you have jdk installed\n","#####################################\n","\n","### please update your relative path while running your code ###\n","temp_airline_textfile2 = r\"./test.txt\"\n","temp_airline_textfile = r\"./flights_data.txt\"\n","# temp_airline_csvfile = r\"./Combined_Flights_2021.csv\"\n","!gdown --id 1JYOFdBSnKQcB24tnQY0HzwsJB4nf2Eek -O Combined_Flights_2021.csv\n","# Update file paths\n","temp_airline_csvfile = r\"/content/Combined_Flights_2021.csv\"  # Path to the downloaded file\n","default_spark_context = \"local[*]\"  # only update if you need\n","#######################################\n","\n","### please don't update these lines ###\n","load_dotenv()\n","airline_textfile = os.getenv(\"AIRLINE_TXT_FILE\", temp_airline_textfile)\n","airline_csvfile = os.getenv(\"AIRLINE_CSV_FILE\", temp_airline_csvfile)\n","spark_context = os.getenv(\"SPARK_CONTEXT\", default_spark_context)\n","#######################################\n","\n","\n","#######################################\n","\n","\n","def co_occurring_airline_pairs_by_origin(flights_data: RDD) -> RDD:\n","    \"\"\"\n","    Takes an RDD that represents the contents of the flights_data from a text file.\n","    Performs a series of MapReduce operations via PySpark to calculate\n","    the number of co-occurring airlines with the same origin airports operating on the same date,\n","    determine count of such occurrences pairwise.\n","    Returns the results as an RDD sorted by the airline pairs alphabetically ascending\n","    (by first and then second value in the pair) with the counts in descending order.\n","\n","    :param flights_data: RDD object of the contents of flights_data\n","    :return: RDD of pairs of airline and the number of occurrences\n","    Example output: [((Airline_A, Airline_B), 4),\n","                     ((Airline_A, Airline_C), 2),\n","                     ((Airline_B, Airline_C), 2)]\n","    \"\"\"\n","\n","    # Step 1: Parse the input data\n","    # Input: RDD of lines (strings)\n","    # Output: RDD[((date, origin), airline)]\n","    parsed_data = flights_data.map(\n","        lambda line: line.strip().split(\",\")  # Split by commas\n","    ).map(\n","        lambda fields: ((fields[0].strip(), fields[2].strip()), fields[1].strip())\n","    )  # Extract ((date, origin), airline)\n","\n","    # Step 2: Group airlines by (date, origin)\n","    # Output: RDD[((date, origin), [airline1, airline2, ...])]\n","    grouped_airlines = parsed_data.groupByKey().mapValues(list)\n","\n","    # Step 3: Generate airline pairs\n","    # Output: RDD[((airline1, airline2), count_per_group)]\n","    airline_pairs = grouped_airlines.flatMap(\n","        lambda x: [\n","            ((min(a, b), max(a, b)), 1)\n","            for a, b in permutations(sorted(set(x[1])), 2)  # Create unique airline pairs\n","            if a != b\n","        ]\n","    )\n","\n","    # Step 4: Count occurrences of each pair across groups\n","    # Output: RDD[((airline1, airline2), total_count)]\n","    pair_counts = airline_pairs.reduceByKey(lambda x, y: x + y)\n","\n","    # Step 5: Sort results\n","    # Sort by pair alphabetically, and by count in descending order\n","    sorted_pairs = pair_counts.sortBy(\n","        lambda x: (-x[1], x[0])\n","    )  # Sort by count descending, then pair alphabetically\n","\n","    return sorted_pairs\n","\n","\n","def air_flights_most_canceled_flights(flights: DataFrame) -> str:\n","    \"\"\"\n","    Finds the airline with the most canceled flights in January 2021.\n","\n","    :param flights: Spark DataFrame containing the flight data\n","    :return: The name of the airline with the most canceled flights in January 2021\n","    \"\"\"\n","    # Filter flights for January 2021 and where Cancelled == 1\n","    jan_canceled_flights = flights.filter((flights.Month == 1) & (flights.Cancelled == 1))\n","\n","    # Group by Airline and count the number of canceled flights\n","    canceled_counts = jan_canceled_flights.groupBy(\"Airline\").count()\n","\n","    # Find the airline with the maximum number of cancellations\n","    most_canceled_airline = canceled_counts.orderBy(canceled_counts[\"count\"].desc()).first()\n","\n","    # Return the name of the airline\n","    return most_canceled_airline[\"Airline\"]\n","\n","\n","def air_flights_diverted_flights(flights: DataFrame) -> int:\n","    \"\"\"\n","    Takes the flight data as a DataFrame and calculates the number of flights that were diverted\n","    between 1st-30th November 2021.\n","\n","    :param flights: Spark DataFrame of the flights CSV file.\n","    :return: The number of diverted flights as an integer.\n","    \"\"\"\n","    # Filter flights for November 2021 where flights were diverted\n","    diverted_flights = flights.filter(\n","        (flights.Month == 11) &  # Month is November\n","        (flights.DayofMonth >= 1) &  # Day of the month is 1 or later\n","        (flights.DayofMonth <= 30) &  # Day of the month is 30 or earlier\n","        (flights.Diverted == 1)  # Flight is diverted\n","    )\n","\n","    # Count the number of diverted flights\n","    num_diverted_flights = diverted_flights.count()\n","\n","    return num_diverted_flights\n","\n","\n","\n","\n","\n","def air_flights_avg_airtime(flights: DataFrame) -> float:\n","    \"\"\"\n","    Takes the flight data as a DataFrame and calculates the average airtime of flights\n","    from Los Angeles, CA to New York, NY.\n","\n","    :param flights: Spark DataFrame of the flights CSV file.\n","    :return: The average airtime as a float number.\n","    \"\"\"\n","    # Filter flights where Origin is Los Angeles, CA and Dest is New York, NY\n","    filtered_flights = flights.filter(\n","        (flights.OriginCityName == \"Los Angeles, CA\") &\n","        (flights.DestCityName == \"New York, NY\")\n","    )\n","\n","    # Ensure AirTime is numeric and calculate the average airtime\n","    avg_airtime = filtered_flights.agg({\"AirTime\": \"avg\"}).first()[0]\n","\n","    # Return the average airtime or 0.0 if no flights match\n","    return float(avg_airtime) if avg_airtime else 0.0\n","\n","\n","\n","\n","def air_flights_missing_departure_time(flights: DataFrame) -> int:\n","    \"\"\"\n","    Takes the flight data as a DataFrame and finds the number of unique days where the departure time (DepTime) is missing.\n","\n","    :param flights: Spark DataFrame of the flights CSV file.\n","    :return: The number of unique days as an integer where DepTime is missing.\n","    \"\"\"\n","    # Filter rows where DepTime is null or missing\n","    missing_dep_time_flights = flights.filter(flights.DepTime.isNull())\n","\n","    # Select unique FlightDate values and count them\n","    unique_days_with_missing_dep_time = missing_dep_time_flights.select(\"FlightDate\").distinct().count()\n","\n","    return unique_days_with_missing_dep_time\n","\n","\n","\n","\n","\n","def main():\n","    # initialize SparkContext and SparkSession\n","    sc = SparkContext(spark_context)\n","    spark = SparkSession.builder.getOrCreate()\n","\n","    print(\"########################## Problem 1 ########################\")\n","    # problem 1: co-occurring operating flights with Spark and MapReduce\n","    # read the file\n","    flights_data = sc.textFile(airline_textfile)\n","    sorted_airline_pairs = co_occurring_airline_pairs_by_origin(flights_data)\n","    sorted_airline_pairs.persist()\n","    for pair, count in sorted_airline_pairs.take(10):\n","       print(f\"{pair}: {count}\")\n","\n","    print(\"########################## Problem 2 ########################\")\n","    # problem 2: PySpark DataFrame operations\n","    # read the file\n","    flights = spark.read.csv(airline_csvfile, header=True, inferSchema=True)\n","    print(\n","        \"Q1:\",\n","        air_flights_most_canceled_flights(flights),\n","        \"had the most canceled flights in January 2021.\",\n","    )\n","\n","    print(\n","        \"Q2:\",\n","        air_flights_diverted_flights(flights),\n","        \"flights were diverted between the period of 1st-30th November 2021.\",\n","    )\n","\n","    print(\n","        \"Q3:\",\n","        air_flights_avg_airtime(flights),\n","        \"is the average airtime for flights that were flying from \"\n","        \"Los Angeles, CA to New York, NY\",\n","    )\n","\n","    print(\n","        \"Q4:\",\n","        air_flights_missing_departure_time(flights),\n","        \"unique dates where departure time (DepTime) was not recorded.\",\n","    )\n","\n","\n","if __name__ == \"__main__\":\n","    main()\n"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"Mg63Shll6dD2","executionInfo":{"status":"error","timestamp":1733087037942,"user_tz":-330,"elapsed":45744,"user":{"displayName":"Praneet Avhad","userId":"11641365352815720975"}},"outputId":"57299258-748c-4db3-8d1f-30da8251e25a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting python-dotenv\n","  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n","Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n","Installing collected packages: python-dotenv\n","Successfully installed python-dotenv-1.0.1\n","Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.5.3)\n","Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n","Requirement already satisfied: gdown in /usr/local/lib/python3.10/dist-packages (5.2.0)\n","Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown) (4.12.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown) (3.16.1)\n","Requirement already satisfied: requests[socks] in /usr/local/lib/python3.10/dist-packages (from gdown) (2.32.3)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gdown) (4.66.6)\n","Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown) (2.6)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.4.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2024.8.30)\n","Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (1.7.1)\n","/usr/local/lib/python3.10/dist-packages/gdown/__main__.py:140: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n","  warnings.warn(\n","Downloading...\n","From (original): https://drive.google.com/uc?id=1JYOFdBSnKQcB24tnQY0HzwsJB4nf2Eek\n","From (redirected): https://drive.google.com/uc?id=1JYOFdBSnKQcB24tnQY0HzwsJB4nf2Eek&confirm=t&uuid=5c14a589-77c8-440f-af40-df480747f796\n","To: /content/Combined_Flights_2021.csv\n","100% 2.21G/2.21G [00:14<00:00, 152MB/s]\n","########################## Problem 1 ########################\n"]},{"output_type":"error","ename":"Py4JJavaError","evalue":"An error occurred while calling o27.partitions.\n: org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: file:/content/flights_data.txt\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:304)\n\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:244)\n\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:332)\n\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:208)\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:294)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:290)\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:294)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:290)\n\tat org.apache.spark.api.java.JavaRDDLike.partitions(JavaRDDLike.scala:61)\n\tat org.apache.spark.api.java.JavaRDDLike.partitions$(JavaRDDLike.scala:61)\n\tat org.apache.spark.api.java.AbstractJavaRDDLike.partitions(JavaRDDLike.scala:45)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: java.io.IOException: Input path does not exist: file:/content/flights_data.txt\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:278)\n\t... 25 more\n","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-b6e7a205691b>\u001b[0m in \u001b[0;36m<cell line: 220>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 221\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-2-b6e7a205691b>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    183\u001b[0m     \u001b[0;31m# read the file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m     \u001b[0mflights_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtextFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mairline_textfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m     \u001b[0msorted_airline_pairs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mco_occurring_airline_pairs_by_origin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflights_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m     \u001b[0msorted_airline_pairs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpersist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mpair\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted_airline_pairs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-2-b6e7a205691b>\u001b[0m in \u001b[0;36mco_occurring_airline_pairs_by_origin\u001b[0;34m(flights_data)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;31m# Step 2: Group airlines by (date, origin)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;31m# Output: RDD[((date, origin), [airline1, airline2, ...])]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m     \u001b[0mgrouped_airlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsed_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupByKey\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapValues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;31m# Step 3: Generate airline pairs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/rdd.py\u001b[0m in \u001b[0;36mgroupByKey\u001b[0;34m(self, numPartitions, partitionFunc)\u001b[0m\n\u001b[1;32m   4177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4178\u001b[0m         \u001b[0mlocally_combined\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcombine\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreservesPartitioning\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4179\u001b[0;31m         \u001b[0mshuffled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlocally_combined\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartitionBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumPartitions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartitionFunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4181\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mgroupByKey\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIterable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mV\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mIterable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mV\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/rdd.py\u001b[0m in \u001b[0;36mpartitionBy\u001b[0;34m(self, numPartitions, partitionFunc)\u001b[0m\n\u001b[1;32m   3844\u001b[0m         \"\"\"\n\u001b[1;32m   3845\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnumPartitions\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3846\u001b[0;31m             \u001b[0mnumPartitions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_defaultReducePartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3847\u001b[0m         \u001b[0mpartitioner\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPartitioner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumPartitions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartitionFunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3848\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartitioner\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mpartitioner\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/rdd.py\u001b[0m in \u001b[0;36m_defaultReducePartitions\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   4865\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaultParallelism\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4866\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4867\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetNumPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4868\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4869\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mlookup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"RDD[Tuple[K, V]]\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mV\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/rdd.py\u001b[0m in \u001b[0;36mgetNumPartitions\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   5451\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5452\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgetNumPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5453\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prev_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5454\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5455\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n","\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o27.partitions.\n: org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: file:/content/flights_data.txt\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:304)\n\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:244)\n\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:332)\n\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:208)\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:294)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:290)\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:294)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:290)\n\tat org.apache.spark.api.java.JavaRDDLike.partitions(JavaRDDLike.scala:61)\n\tat org.apache.spark.api.java.JavaRDDLike.partitions$(JavaRDDLike.scala:61)\n\tat org.apache.spark.api.java.AbstractJavaRDDLike.partitions(JavaRDDLike.scala:45)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: java.io.IOException: Input path does not exist: file:/content/flights_data.txt\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:278)\n\t... 25 more\n"]}],"source":["!pip install python-dotenv\n","!pip install pyspark\n","!pip install gdown\n","\n","import os\n","from itertools import permutations\n","from dotenv import load_dotenv\n","from pyspark import RDD, SparkContext\n","from pyspark.sql import DataFrame, SparkSession\n","\n","### install dependency ###\n","# pip install python-dotenv\n","# pip install pyspark # make sure you have jdk installed\n","#####################################\n","\n","### please update your relative path while running your code ###\n","temp_airline_textfile2 = r\"./test.txt\"\n","temp_airline_textfile = r\"./flights_data.txt\"\n","# temp_airline_csvfile = r\"./Combined_Flights_2021.csv\"\n","!gdown --id 1JYOFdBSnKQcB24tnQY0HzwsJB4nf2Eek -O Combined_Flights_2021.csv\n","# Update file paths\n","temp_airline_csvfile = r\"/content/Combined_Flights_2021.csv\"  # Path to the downloaded file\n","default_spark_context = \"local[*]\"  # only update if you need\n","#######################################\n","\n","### please don't update these lines ###\n","load_dotenv()\n","airline_textfile = os.getenv(\"AIRLINE_TXT_FILE\", temp_airline_textfile)\n","airline_csvfile = os.getenv(\"AIRLINE_CSV_FILE\", temp_airline_csvfile)\n","spark_context = os.getenv(\"SPARK_CONTEXT\", default_spark_context)\n","#######################################\n","\n","\n","#######################################\n","\n","\n","def co_occurring_airline_pairs_by_origin(flights_data: RDD) -> RDD:\n","    \"\"\"\n","    Takes an RDD that represents the contents of the flights_data from a text file.\n","    Performs a series of MapReduce operations via PySpark to calculate\n","    the number of co-occurring airlines with the same origin airports operating on the same date,\n","    determine count of such occurrences pairwise.\n","    Returns the results as an RDD sorted by the airline pairs alphabetically ascending\n","    (by first and then second value in the pair) with the counts in descending order.\n","\n","    :param flights_data: RDD object of the contents of flights_data\n","    :return: RDD of pairs of airline and the number of occurrences\n","    Example output: [((Airline_A, Airline_B), 4),\n","                     ((Airline_A, Airline_C), 2),\n","                     ((Airline_B, Airline_C), 2)]\n","    \"\"\"\n","\n","    # Step 1: Parse the input data\n","    # Input: RDD of lines (strings)\n","    # Output: RDD[((date, origin), airline)]\n","    parsed_data = flights_data.map(\n","        lambda line: line.strip().split(\",\")  # Split by commas\n","    ).map(\n","        lambda fields: ((fields[0].strip(), fields[2].strip()), fields[1].strip())\n","    )  # Extract ((date, origin), airline)\n","\n","    # Step 2: Group airlines by (date, origin)\n","    # Output: RDD[((date, origin), [airline1, airline2, ...])]\n","    grouped_airlines = parsed_data.groupByKey().mapValues(list)\n","\n","    # Step 3: Generate airline pairs\n","    # Output: RDD[((airline1, airline2), count_per_group)]\n","    airline_pairs = grouped_airlines.flatMap(\n","        lambda x: [\n","            ((min(a, b), max(a, b)), 1)\n","            for a, b in permutations(sorted(set(x[1])), 2)  # Create unique airline pairs\n","            if a != b\n","        ]\n","    )\n","\n","    # Step 4: Count occurrences of each pair across groups\n","    # Output: RDD[((airline1, airline2), total_count)]\n","    pair_counts = airline_pairs.reduceByKey(lambda x, y: x + y)\n","\n","    # Step 5: Sort results\n","    # Sort by pair alphabetically, and by count in descending order\n","    sorted_pairs = pair_counts.sortBy(\n","        lambda x: (-x[1], x[0])\n","    )  # Sort by count descending, then pair alphabetically\n","\n","    return sorted_pairs\n","\n","\n","def air_flights_most_canceled_flights(flights: DataFrame) -> str:\n","    \"\"\"\n","    Finds the airline with the most canceled flights in January 2021.\n","\n","    :param flights: Spark DataFrame containing the flight data\n","    :return: The name of the airline with the most canceled flights in January 2021\n","    \"\"\"\n","    # Filter flights for January 2021 and where Cancelled == 1\n","    jan_canceled_flights = flights.filter((flights.Month == 1) & (flights.Cancelled == 1))\n","\n","    # Group by Airline and count the number of canceled flights\n","    canceled_counts = jan_canceled_flights.groupBy(\"Airline\").count()\n","\n","    # Find the airline with the maximum number of cancellations\n","    most_canceled_airline = canceled_counts.orderBy(canceled_counts[\"count\"].desc()).first()\n","\n","    # Return the name of the airline\n","    return most_canceled_airline[\"Airline\"]\n","\n","\n","def air_flights_diverted_flights(flights: DataFrame) -> int:\n","    \"\"\"\n","    Takes the flight data as a DataFrame and calculates the number of flights that were diverted\n","    between 1st-30th November 2021.\n","\n","    :param flights: Spark DataFrame of the flights CSV file.\n","    :return: The number of diverted flights as an integer.\n","    \"\"\"\n","    # Filter flights for November 2021 where flights were diverted\n","    diverted_flights = flights.filter(\n","        (flights.Month == 11) &  # Month is November\n","        (flights.DayofMonth >= 1) &  # Day of the month is 1 or later\n","        (flights.DayofMonth <= 30) &  # Day of the month is 30 or earlier\n","        (flights.Diverted == 1)  # Flight is diverted\n","    )\n","\n","    # Count the number of diverted flights\n","    num_diverted_flights = diverted_flights.count()\n","\n","    return num_diverted_flights\n","\n","\n","\n","\n","\n","def air_flights_avg_airtime(flights: DataFrame) -> float:\n","    \"\"\"\n","    Takes the flight data as a DataFrame and calculates the average airtime of flights\n","    from Los Angeles, CA to New York, NY.\n","\n","    :param flights: Spark DataFrame of the flights CSV file.\n","    :return: The average airtime as a float number.\n","    \"\"\"\n","    # Filter flights where Origin is Los Angeles, CA and Dest is New York, NY\n","    filtered_flights = flights.filter(\n","        (flights.OriginCityName == \"Los Angeles, CA\") &\n","        (flights.DestCityName == \"New York, NY\")\n","    )\n","\n","    # Ensure AirTime is numeric and calculate the average airtime\n","    avg_airtime = filtered_flights.agg({\"AirTime\": \"avg\"}).first()[0]\n","\n","    # Return the average airtime or 0.0 if no flights match\n","    return float(avg_airtime) if avg_airtime else 0.0\n","\n","\n","\n","\n","def air_flights_missing_departure_time(flights: DataFrame) -> int:\n","    \"\"\"\n","    Takes the flight data as a DataFrame and finds the number of unique days where the departure time (DepTime) is missing.\n","\n","    :param flights: Spark DataFrame of the flights CSV file.\n","    :return: The number of unique days as an integer where DepTime is missing.\n","    \"\"\"\n","    # Filter rows where DepTime is null or missing\n","    missing_dep_time_flights = flights.filter(flights.DepTime.isNull())\n","\n","    # Select unique FlightDate values and count them\n","    unique_days_with_missing_dep_time = missing_dep_time_flights.select(\"FlightDate\").distinct().count()\n","\n","    return unique_days_with_missing_dep_time\n","\n","\n","\n","\n","\n","def main():\n","    # initialize SparkContext and SparkSession\n","    sc = SparkContext(spark_context)\n","    spark = SparkSession.builder.getOrCreate()\n","\n","    print(\"########################## Problem 1 ########################\")\n","    # problem 1: co-occurring operating flights with Spark and MapReduce\n","    # read the file\n","    flights_data = sc.textFile(airline_textfile)\n","    sorted_airline_pairs = co_occurring_airline_pairs_by_origin(flights_data)\n","    sorted_airline_pairs.persist()\n","    for pair, count in sorted_airline_pairs.take(10):\n","       print(f\"{pair}: {count}\")\n","\n","    print(\"########################## Problem 2 ########################\")\n","    # problem 2: PySpark DataFrame operations\n","    # read the file\n","    flights = spark.read.csv(airline_csvfile, header=True, inferSchema=True)\n","    print(\n","        \"Q1:\",\n","        air_flights_most_canceled_flights(flights),\n","        \"had the most canceled flights in January 2021.\",\n","    )\n","\n","    print(\n","        \"Q2:\",\n","        air_flights_diverted_flights(flights),\n","        \"flights were diverted between the period of 1st-30th November 2021.\",\n","    )\n","\n","    print(\n","        \"Q3:\",\n","        air_flights_avg_airtime(flights),\n","        \"is the average airtime for flights that were flying from \"\n","        \"Los Angeles, CA to New York, NY\",\n","    )\n","\n","    print(\n","        \"Q4:\",\n","        air_flights_missing_departure_time(flights),\n","        \"unique dates where departure time (DepTime) was not recorded.\",\n","    )\n","\n","\n","if __name__ == \"__main__\":\n","    main()\n"]}]}